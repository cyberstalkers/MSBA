{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSBA7011 Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: MNIST  3-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DATA/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting DATA/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting DATA/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting DATA/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"DATA/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get training and testing data set\n",
    "train_x = mnist.train.images\n",
    "train_y = mnist.train.labels\n",
    "\n",
    "test_x = mnist.test.images\n",
    "test_y = mnist.test.labels\n",
    "\n",
    "# Define some parameters\n",
    "num_cls = 10\n",
    "num_features = 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow Setup\n",
    "\n",
    "# Some parameters\n",
    "num_iter = 10000\n",
    "batch_size = 100\n",
    "num_hidden_1 = 256\n",
    "num_hidden_2 = 128\n",
    "\n",
    "# Placeholder variables\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, num_features]) \n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[None, num_cls]) \n",
    "\n",
    "# Input -- Hidden 1\n",
    "W1 = tf.Variable(tf.truncated_normal(shape=[num_features, num_hidden_1], stddev=0.06))\n",
    "b1 = tf.Variable(tf.constant(0.1,shape=[num_hidden_1]))\n",
    "# Hidden 1 -- Hidden 2\n",
    "W2 = tf.Variable(tf.truncated_normal(shape=[num_hidden_1, num_hidden_2], stddev=0.06))\n",
    "b2 = tf.Variable(tf.constant(0.1,shape=[num_hidden_2]))\n",
    "# Hidden 2 -- Output\n",
    "W3 = tf.Variable(tf.truncated_normal(shape=[num_hidden_2, num_cls], stddev=0.06))\n",
    "b3 = tf.Variable(tf.zeros(shape=[num_cls]))\n",
    "\n",
    "\n",
    "# Model\n",
    "h1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    "logits = tf.matmul(h2, W3) + b3\n",
    "y_pred = tf.nn.softmax(logits) # to know prob distribution\n",
    "\n",
    "# Cost function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=logits)) # the function avoids log(0) situation\n",
    "\n",
    "# Optimization\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow Training\n",
    "\n",
    "# Session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize variables\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Run TensorFlow with Stochastic Gradient Descent\n",
    "for ik in range(num_iter):\n",
    "  batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "  sess.run(optimizer, feed_dict={x: batch_x, y_true: batch_y})\n",
    "\n",
    "# for ik in range(num_iter):\n",
    "#     sess.run(optimizer, feed_dict={x: train_x, y_true: train_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.979\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "results_pred = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(results_pred, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={x: test_x, y_true: test_y}))\n",
    "\n",
    "# Around 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Close Session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Iris 4-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw things on the Jupyter Notebook\n",
    "%matplotlib inline  \n",
    "\n",
    "# Importing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from six.moves.urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data sets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "# If the training and test sets aren't stored locally, download them.\n",
    "if not os.path.exists(IRIS_TRAINING):\n",
    "  raw = urlopen(IRIS_TRAINING_URL).read()\n",
    "  with open(IRIS_TRAINING,'wb') as f:\n",
    "    f.write(raw)\n",
    "\n",
    "if not os.path.exists(IRIS_TEST):\n",
    "  raw = urlopen(IRIS_TEST_URL).read()\n",
    "  with open(IRIS_TEST,'wb') as f:\n",
    "    f.write(raw)\n",
    "    \n",
    "# Load datasets.\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TRAINING,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)\n",
    "test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TEST,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Iris Dataset into Python\n",
    "\n",
    "use pandas to load the Iris data into Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load datasets via Pandas.\n",
    "filename_iris_train = \"iris_training.csv\"\n",
    "filename_iris_test = \"iris_test.csv\"\n",
    "\n",
    "iris_train = pd.read_csv(filename_iris_train)\n",
    "iris_test = pd.read_csv(filename_iris_test)\n",
    "\n",
    "# Convert to One-hot encoding\n",
    "iris_train_onehot = pd.get_dummies(iris_train,columns=['virginica'])\n",
    "iris_test_onehot = pd.get_dummies(iris_test,columns=['virginica'])\n",
    "\n",
    "# Training data\n",
    "iris_train_x = iris_train_onehot.values[:,:-3]\n",
    "iris_train_y = iris_train_onehot.values[:,-3:]\n",
    "# Test data\n",
    "iris_test_x = iris_test_onehot.values[:,:-3]\n",
    "iris_test_y = iris_test_onehot.values[:,-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120, 3)\n",
      "(30, 4)\n",
      "(30, 3)\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "print(iris_train_x.shape)\n",
    "print(iris_train_y.shape)\n",
    "\n",
    "print(iris_test_x.shape)\n",
    "print(iris_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on the above outputs, define the following parameters\n",
    "num_cls = 3\n",
    "num_features = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling via Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow Setup\n",
    "\n",
    "# Some parameters\n",
    "num_iter = 5000\n",
    "# Number of hidden units in the first hidden layer\n",
    "num_h1 = 10\n",
    "# Number of hidden units in the second hidden layer\n",
    "num_h2 = 20\n",
    "# Number of hidden units in the third hidden layer\n",
    "num_h3 = 10\n",
    "\n",
    "# Placeholder variables\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, num_features]) \n",
    "y_true = tf.placeholder(dtype=tf.float32, shape=[None, num_cls]) \n",
    "\n",
    "# Input -- Hidden 1\n",
    "W1 = tf.Variable(tf.truncated_normal(shape=[num_features, num_h1], stddev=0.06))\n",
    "b1 = tf.Variable(tf.constant(0.1,shape=[num_h1]))\n",
    "# Hidden 1 -- Hidden 2\n",
    "W2 = tf.Variable(tf.truncated_normal(shape=[num_h1, num_h2], stddev=0.06))\n",
    "b2 = tf.Variable(tf.constant(0.1,shape=[num_h2]))\n",
    "# Hidden 2 -- Hidden 3\n",
    "W3 = tf.Variable(tf.truncated_normal(shape=[num_h2, num_h3], stddev=0.06))\n",
    "b3 = tf.Variable(tf.constant(0.1,shape=[num_h3]))\n",
    "# Hidden 3 -- Output\n",
    "W4 = tf.Variable(tf.truncated_normal(shape=[num_h3, num_cls], stddev=0.06))\n",
    "b4 = tf.Variable(tf.zeros(shape=[num_cls]))\n",
    "\n",
    "\n",
    "# Model\n",
    "h1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    "h3 = tf.nn.relu(tf.matmul(h2, W3) + b3)\n",
    "logits = tf.matmul(h3, W4) + b4\n",
    "y_pred = tf.nn.softmax(logits) # to know prob distribution\n",
    "\n",
    "\n",
    "# Cost function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=logits))\n",
    "\n",
    "# Optimization\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow Train\n",
    "\n",
    "# Session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize variables\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Run TensorFlow\n",
    "for ik in range(num_iter):\n",
    "    sess.run(optimizer, feed_dict={x: iris_train_x, y_true: iris_train_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.966667\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "results_pred = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(results_pred, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={x: iris_test_x, y_true: iris_test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Close Session\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
